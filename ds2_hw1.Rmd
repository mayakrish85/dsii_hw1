---
title: "Data Science II HW 1"
author: "Maya Krishnamoorthy"
date: "2025-02-22"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(glmnet)
library(caret)
library(tidymodels)
library(corrplot)
library(ggplot2)
library(plotmo)
library(ggrepel)
```

Read and prep CSV files.

```{r}
train_df = read_csv("housing_training.csv") %>% janitor::clean_names()
test_df = read_csv("housing_test.csv") %>% janitor::clean_names()
```
Response variable: `sale_price`

## Part a)

**Fit a lasso model on the training data. Report the selected tuning parameter and the test error. When the 1SE rule is applied, how many predictors are included in the model?**

### Minimizing CV error

```{r}
# Using glmnet
ctrl1 = trainControl(method = "repeatedcv",
                     number = 10,
                     repeats = 5,
                     selectionFunction = "best")

set.seed(2025)
lasso.fit =
  train(sale_price ~ .,
        data = train_df,
        method = "glmnet",
        tuneGrid = expand.grid(alpha = 1, lambda = exp(seq(6, 0, length = 100))),
        trControl = ctrl1)

plot(lasso.fit, xTrans = log)

lasso.fit$bestTune

# coefficients in the final model
coef(lasso.fit$finalModel, lasso.fit$bestTune$lambda)

# test error
predictions = predict(lasso.fit, newdata = test_df)
mse = mean((predictions - pull(test_df, "sale_price"))^2)
```


The tuning parameter that minimizes CV error is 61.6339. The MSE associated with this model is 440335429.

### Minimizing for 1SE

```{r}
ctrl2 = 
  trainControl(method = "repeatedcv",
               number = 10,
               repeats = 5,
               selectionFunction = "oneSE")

set.seed(2025)

lasso.fit_1se = 
  train(sale_price ~ .,
        data = train_df,
        method = "glmnet",
        tuneGrid = expand.grid(alpha = 1, lambda = exp(seq(6, 0, length = 100))),
        trControl = ctrl2)

# Best lambda for 1SE
lasso.fit_1se$bestTune

# Getting number of predictors
coeff = coef(lasso.fit_1se$finalModel, lasso.fit_1se$bestTune$lambda)
length(which(coeff != 0)) - 1

# MSE
predictions = predict(lasso.fit_1se, newdata = test_df)
mse = mean((predictions - pull(test_df, "sale_price"))^2); mse
```

Using the 1SE rule, the optimal tuning parameter is 403.4288. The MSE is 420726548. There are 36 (non-zero) predictors in this model.

## Part b)

**Fit an elastic net model on the training data. Report the selected tuning parameters and the test error. Is it possible to apply the 1SE rule to select the tuning parameters for elastic net? If the 1SE rule is applicable, implement it to select the tuning parameters. If not, explain why.**

```{r}
set.seed(2025)

enet.fit <- train(sale_price ~ .,
                  data = train_df,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 20), 
                                         lambda = exp(seq(10, 0, length = 100))),
                  trControl = ctrl1)
enet.fit$bestTune

myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

plot(enet.fit, par.settings = myPar, xTrans = log)

# coefficients in the final model
coef(enet.fit$finalModel, enet.fit$bestTune$lambda)

# MSE
enet.pred = predict(enet.fit, newdata = test_df)
mean((enet.pred - pull(test_df, "sale_price"))^2)
```

The optimal tuning parameters for the elastic net model are lambda = 580.3529 and alpha = 0.05263158. The test error is 438502352.

```{r}
# Trying 1SE method
set.seed(2025)
enet.fit_1se <- train(sale_price ~ .,
                  data = train_df,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 20), 
                                         lambda = exp(seq(10, 0, length = 100))),
                  trControl = ctrl2)
enet.fit_1se$bestTune

# MSE
enet.pred_1se = predict(enet.fit_1se, newdata = test_df)
mean((enet.pred_1se - pull(test_df, "sale_price"))^2)
```
Using 1SE, the resulting tuning parameters are alpha = 0 and lambda = 6554.314. The resulting test error is 426591709. Since the test errors are similar for both 1SE and CV methods, we can go ahead and use the 1SE method as well. **CHECK THIS**

## Part c)

**Fit a partial least squares model on the training data and report the test error. How
many components are included in your model?**

```{r}
set.seed(2025)

pls_fit = train(sale_price ~ .,
                data = train_df,
                method = "pls",
                tuneGrid = data.frame(ncomp = 1:(ncol(train_df)-1)),
                trControl = ctrl1,
                preProcess = c("center", "scale"))

pred.pls = predict(pls_fit, newdata = test_df)
mean((pred.pls - pull(test_df, "sale_price"))^2)
ggplot(pls_fit, highlight = TRUE)

```

The final model used 12 components.

## Part d)

**Choose the best model for predicting the response and explain your choice.**

```{r}
resamp = resamples(list(
  lasso = lasso.fit,
  lasso_1SE = lasso.fit_1se,
  elastic_net = enet.fit,
  elastic_net_1SE = enet.fit_1se,
  pls = pls_fit))

summary(resamp)

bwplot(resamp, metric = "RMSE")
```

Elastic net (minimizing CV error) performs the best out of the 5 models with regards to all measures of test error, as seen in the graph above and the list of resamples. Based on RMSE, I would conclude that the elastic net model is the best-performing.

## Part e)

**If R package `caret` was used for the lasso in (a), retrain this model using R package `glmnet`, and vice versa. Compare the selected tuning parameters between the two software approaches. Should there be discrepancies in the chosen parameters, discuss potential reasons for these differences.**

```{r}
x = model.matrix(sale_price ~ ., train_df)[,-1] # convert into a binary indicator variable
# vector of response
y = pull(train_df, "sale_price")

cv.lasso = cv.glmnet(x, y, 
                      alpha = 1, 
                      lambda = exp(seq(6, -5, length = 100)))

cv.lasso$lambda.min
cv.lasso$lambda.1se

plot(cv.lasso)
plot_glmnet(cv.lasso$glmnet.fit)
```


